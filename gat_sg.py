# -*- coding: utf-8 -*-
"""GAT SG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DfiwbyRWiUQWhRVksL6g-7sZx3qQxiyf
"""

import torch
print(torch.__version__)
print(torch.version.cuda)

from utils.dataset import WordNet18
wn18graph = WordNet18(root='data/wordnet18my')
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
print(wordnet.synset_from_pos_and_offset('n', 3964744))

from torchtext.vocab import GloVe
glove = GloVe('6B', dim=50)

"""# The Annotated GAT (PPI)

This notebook is the 2nd part of the series, please check out **"The Annotated GAT (Cora)"** notebook for a more gentle introduction to GAT. ‚ù§Ô∏è

The idea of this notebook is to explain how you can use GAT in an **inductive setting**. 

I'll be using the **PPI (protein-protein interaction) dataset** in this notebook.

Here is a representation of the 3D structure of the protein [myoglobin](https://en.wikipedia.org/wiki/Protein) (not like you need to know anything about proteins in order to follow along this notebook it's just that they are beautiful and I love them üçó‚ù§Ô∏è)!

<img src="data/readme_pics/protein.png" alt="protein schematic" align="center"/> <br/>

In this notebook you'll get the answers to these questions:

‚úÖ How to load and visualize the PPI dataset? <br/>
‚úÖ How to train/use GAT on PPI (multi-label classification problem)? <br/>
‚úÖ How to visualize different GAT's properties? (mainly attention) <br/>

Awesome, let's start!
"""

# I always like to structure my imports into Python's native libs,
# stuff I installed via conda/pip and local file imports (but we don't have those here)

import json
import os
import enum

# Visualization related imports
import matplotlib.pyplot as plt
import networkx as nx
from networkx.readwrite import json_graph
import igraph as ig

# Main computation libraries
import numpy as np

# Deep learning related imports
import torch
from torch.utils.data import DataLoader, Dataset

"""
    Contains constants needed for data loading and visualization.

"""


# Supported datasets - only PPI in this notebook
class DatasetType(enum.Enum):
    PPI = 0

    
class GraphVisualizationTool(enum.Enum):
    IGRAPH = 0


# We'll be dumping and reading the data from this directory
DATA_DIR_PATH = os.path.join(os.getcwd(), 'data')
PPI_PATH = os.path.join(DATA_DIR_PATH, 'ppi')
PPI_URL = 'https://data.dgl.ai/dataset/ppi.zip'  # preprocessed PPI data from Deep Graph Library

#
# PPI specific constants
#

PPI_NUM_INPUT_FEATURES = 50
PPI_NUM_CLASSES = 121

"""Note that some parts of this notebook overlap with "The Annotated GAT (Cora)" notebook, so you may see some redundancy. üôè

With that out of the way we've got the level 1 unlocked (Data üìú). üòç Let's go!

# Part 1: Understanding your data (become One with the data üìú‚ù§Ô∏è)

I'll be using the PPI dataset as the running example in this notebook.

Having said that, you may wonder, what's the difference between `transductive` and `inductive` setting? If you're not familiar with GNNs this may appear as a weird concept. But it's quite simple actually.

**Transductive** - you have a single graph (like Cora) you split some **nodes** (and not graphs) into train/val/test training sets. While you're training you'll be using only the labels from your training nodes. BUT. During the forward prop, by the nature of how spatial GNNs work, you'll be aggregating the feature vectors from your neighbors and **some of them may belong to val or even test sets!** The main point is - you **ARE NOT** using their label information but you **ARE** using the structural information and their features.

**Inductive** - you're probably much more familiar with this one if you come from the computer vision or NLP background. You have a set of training graphs, a separate set of val graphs and of course a separate set of test graphs.

Having explained that let's jump into the code and let's load and visualize PPI.
"""

# First let's define this simple function for loading PPI's graph data

def json_read(path):
    with open(path, 'r') as file:
        data = json.load(file)

    return data

"""Now let's see how we can load PPI!"""

from torch.hub import download_url_to_file
import zipfile

def load_graph_data(training_config, device):
    dataset_name = training_config['dataset_name'].lower()
    should_visualize = training_config['should_visualize']

    if dataset_name == DatasetType.PPI.name.lower():  # Protein-Protein Interaction dataset

        # Instead of checking PPI in, I'd rather download it on-the-fly the first time it's needed (lazy execution ^^)
        if not os.path.exists(PPI_PATH):  # download the first time this is ran
            os.makedirs(PPI_PATH)

            # Step 1: Download the ppi.zip (contains the PPI dataset)
            zip_tmp_path = os.path.join(PPI_PATH, 'ppi.zip')
            download_url_to_file(PPI_URL, zip_tmp_path)

            # Step 2: Unzip it
            with zipfile.ZipFile(zip_tmp_path) as zf:
                zf.extractall(path=PPI_PATH)
            print(f'Unzipping to: {PPI_PATH} finished.')

            # Step3: Remove the temporary resource file
            os.remove(zip_tmp_path)
            print(f'Removing tmp file {zip_tmp_path}.')

        # Collect train/val/test graphs here
        edge_index_list = []
        node_features_list = []
        node_labels_list = []

        # Dynamically determine how many graphs we have per split (avoid using constants when possible)
        num_graphs_per_split_cumulative = [0]

        # Small optimization "trick" since we only need test in the playground.py
        splits = ['test'] if training_config['ppi_load_test_only'] else ['train', 'valid', 'test']

        for split in splits:
            # PPI has 50 features per node, it's a combination of positional gene sets, motif gene sets,
            # and immunological signatures - you can treat it as a black box (I personally have a rough understanding)
            # shape = (NS, 50) - where NS is the number of (N)odes in the training/val/test (S)plit
            # Note: node features are already preprocessed
            node_features = np.load(os.path.join(PPI_PATH, f'{split}_feats.npy'))

            # PPI has 121 labels and each node can have multiple labels associated (gene ontology stuff)
            # SHAPE = (NS, 121)
            node_labels = np.load(os.path.join(PPI_PATH, f'{split}_labels.npy'))

            # Graph topology stored in a special nodes-links NetworkX format
            nodes_links_dict = json_read(os.path.join(PPI_PATH, f'{split}_graph.json'))
            # PPI contains undirected graphs with self edges - 20 train graphs, 2 validation graphs and 2 test graphs
            # The reason I use a NetworkX's directed graph is because we need to explicitly model both directions
            # because of the edge index and the way GAT implementation #3 works
            collection_of_graphs = nx.DiGraph(json_graph.node_link_graph(nodes_links_dict))
            # For each node in the above collection, ids specify to which graph the node belongs to
            graph_ids = np.load(os.path.join(PPI_PATH, F'{split}_graph_id.npy'))
            num_graphs_per_split_cumulative.append(num_graphs_per_split_cumulative[-1] + len(np.unique(graph_ids)))

            # Split the collection of graphs into separate PPI graphs
            for graph_id in range(np.min(graph_ids), np.max(graph_ids) + 1):
                mask = graph_ids == graph_id  # find the nodes which belong to the current graph (identified via id)
                graph_node_ids = np.asarray(mask).nonzero()[0]
                graph = collection_of_graphs.subgraph(graph_node_ids)  # returns the induced subgraph over these nodes
                print(f'Loading {split} graph {graph_id} to CPU. '
                      f'It has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.')

                # shape = (2, E) - where E is the number of edges in the graph
                # Note: leaving the tensors on CPU I'll load them to GPU in the training loop on-the-fly as VRAM
                # is a scarcer resource than CPU's RAM and the whole PPI dataset can't fit during the training.
                edge_index = torch.tensor(list(graph.edges), dtype=torch.long).transpose(0, 1).contiguous()
                edge_index = edge_index - edge_index.min()  # bring the edges to [0, num_of_nodes] range
                edge_index_list.append(edge_index)
                # shape = (N, 50) - where N is the number of nodes in the graph
                node_features_list.append(torch.tensor(node_features[mask], dtype=torch.float))
                # shape = (N, 121), BCEWithLogitsLoss doesn't require long/int64 so saving some memory by using float32
                node_labels_list.append(torch.tensor(node_labels[mask], dtype=torch.float))

                if should_visualize:
                    plot_in_out_degree_distributions(edge_index.numpy(), graph.number_of_nodes(), dataset_name)
                    visualize_graph(edge_index.numpy(), node_labels[mask], dataset_name)

        #
        # Prepare graph data loaders
        #

        # Optimization, do a shortcut in case we only need the test data loader
        if training_config['ppi_load_test_only']:
            data_loader_test = GraphDataLoader(
                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                batch_size=training_config['batch_size'],
                shuffle=False
            )
            return data_loader_test
        else:

            data_loader_train = GraphDataLoader(
                node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
                batch_size=training_config['batch_size'],
                shuffle=True
            )

            data_loader_val = GraphDataLoader(
                node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
                node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
                edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
                batch_size=training_config['batch_size'],
                shuffle=False  # no need to shuffle the validation and test graphs
            )

            data_loader_test = GraphDataLoader(
                node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
                node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
                edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
                batch_size=training_config['batch_size'],
                shuffle=False
            )

            return data_loader_train, data_loader_val, data_loader_test
    else:
        raise Exception(f'{dataset_name} not yet supported.')

from nltk.corpus import wordnet
for i in list(map(lambda x: (x, x.lemma_names(lang='eng')), wordnet.synsets('white'))):
    print(i)

from utils.dataset import WordNet18
dataset = WordNet18(root='data/wordnet18')
data = dataset[0]

import pydot
from IPython.display import SVG, display

def view_pydot(pdot):
    plt = SVG(pdot.create_svg())
    display(plt)

def extend_wn_neighbors(g_node_synsets, g_edge_index):
    graph = pydot.Dot("my_graph", graph_type="digraph")
    for node_synset in g_node_synsets:
        extend_wn_neighbors_node(node_synset, g_edge_index, graph)
    graph.write_svg('graph.svg')

def extend_wn_neighbors_node(node_synset, g_edge_index, graph):
    next_idx = len(synsets_to_classes)
    start = node_synset

    node_synsets_extension = []
    node_synset_to_node = {}
    edge_index_extension = [
        [],
        []
    ]

    if start not in dataset.entity2id:
        return
    start_id = dataset.entity2id[start]
    start_n = pydot.Node(start)
    graph.add_node(start_n)

    nodes = {start_id: start_n}

    for (src, dst), edg in zip(data.edge_index.T, data.edge_type):
        if src == start_id:
            src_n = start_n
            dest_id = dst.item()
            dest = dataset.id2entity[dest_id]
            dest_n = nodes.get(dest_id)
            if dest_n is None:
                dest_n = nodes[dest_id] = pydot.Node(dest)
                graph.add_node(dest_n)
            edge_id = edg.item()
            edge = dataset.id2edge[edge_id]
            edge_index_extension[0].append(class_of(start))
            edge_index_extension[1].append(class_of(edge))
            edge_index_extension[0].append(class_of(edge))
            edge_index_extension[1].append(class_of(dest))
            graph.add_edge(pydot.Edge(src_n, dest_n, label=edge))
        elif dst == start_id:
            dest_n = start_n
            src_id = src.item()
            src = dataset.id2entity[src_id]
            src_n = nodes.get(src_id)
            if src_n is None:
                src_n = nodes[src_id] = pydot.Node(src)
                graph.add_node(src_n)
            edge_id = edg.item()
            edge = dataset.id2edge[edge_id]
            edge_index_extension[0].append(class_of(src))
            edge_index_extension[1].append(class_of(edge))
            edge_index_extension[0].append(class_of(edge))
            edge_index_extension[1].append(class_of(start))
            graph.add_edge(pydot.Edge(src_n, dest_n, label=edge))
    g_edge_index[0] = g_edge_index[0] + edge_index_extension[0]
    g_edge_index[1] = g_edge_index[1] + edge_index_extension[1]
    return graph
#graph = extend_wn_neighbors(None)
#view_pydot(graph)

# classes = [
#     'none',
#     'doctor',
#     'farmer',
#     'waiter',
# ]
# words_to_classes = {w: i for i, w in enumerate(classes)}

edge_index_list = []
node_features_list = []
node_labels_list = []

node_synsets_list = []
synsets_to_classes = {}
classes_to_synsets = []

def get_synset_class(synsets_to_classes, classes_to_synsets, node_synset):
    next_idx = len(synsets_to_classes)
    if node_synset not in synsets_to_classes:
        synsets_to_classes[node_synset] = next_idx
        classes_to_synsets.append(node_synset)
    return synsets_to_classes[node_synset]

def class_of(s):
    return get_synset_class(synsets_to_classes, classes_to_synsets, s)

def init_classes(g_node_synsets):
    return [class_of(s) for s in g_node_synsets]

def make_graph(g_node_synsets, g_node_features_words, g_node_labels_words, g_edge_index):
    g_node_features = torch.stack([
        glove[word] for word in g_node_features_words
    ])
    #g_node_labels = torch.zeros(g_node_features.shape[0], len(classes))
    #g_node_labels[range(g_node_features.shape[0]), [words_to_classes[w] for w in g_node_labels_words]] = 1
    g_node_labels = torch.tensor(init_classes(g_node_synsets))
    return g_edge_index, g_node_features, g_node_labels

def append_graph(g_node_synsets, *args):
    e, f, l = make_graph(g_node_synsets, *args)
    edge_index_list.append(e)
    node_features_list.append(f)
    node_labels_list.append(l)

    # collect synsets
    # i = len(synsets_to_classes)  # next class idx
    # for s in g_node_synsets:
    #     if s not in synsets_to_classes:
    #         synsets_to_classes[s] = i
    #         classes_to_synsets.append(s)
    #         i += 1

# Graph 1        # 0              1            2                3            4            5          6
g_node_synsets = ['person.n.01', 'hold.v.02', 'notebook.n.01', 'wear.v.01', 'coat.n.01', 'be.v.01', 'white.a.01']
g_node_features_words = ['person', 'holds', 'notebook', 'wears', 'coat', 'is', 'white']
g_node_labels_words = ['doctor', 'none', 'none', 'none', 'none', 'none', 'none']
g_edge_index = [
    [0, 1, 0, 3, 4, 5],
    [1, 2, 3, 4, 5, 6],
]
init_classes(g_node_synsets)
extend_wn_neighbors(g_node_synsets, g_edge_index)
g_edge_index = torch.tensor(g_edge_index, dtype=torch.long)
append_graph(g_node_synsets, g_node_features_words, g_node_labels_words, g_edge_index)
exit(0)

# Graph 2        # 0              1            2
g_node_synsets = ['person.n.01', 'hold.v.02', 'plant.n.02']
g_node_features_words = ['person', 'holds', 'plant']
g_node_labels_words = ['farmer', 'none', 'none']
g_edge_index = [
    [0, 1],
    [1, 2],
]
init_classes(g_node_synsets)
extend_wn_neighbors(g_node_synsets, g_edge_index)
g_edge_index = torch.tensor(g_edge_index, dtype=torch.long)
append_graph(g_node_synsets, g_node_features_words, g_node_labels_words, g_edge_index)

# Graph 3        # 0              1            2             3               4
g_node_synsets = ['person.n.01', 'hold.v.02', 'plate.n.04', 'contain.v.01', 'food.v.01']
g_node_features_words = ['person', 'holds', 'plate', 'contains', 'food']
g_node_labels_words = ['waiter', 'none', 'none', 'none', 'none']
g_edge_index = [
    [0, 1, 2, 3],
    [1, 2, 3, 4],
]
init_classes(g_node_synsets)
extend_wn_neighbors(g_node_synsets, g_edge_index)
g_edge_index = torch.tensor(g_edge_index, dtype=torch.long)
append_graph(g_node_synsets, g_node_features_words, g_node_labels_words, g_edge_index)

# Graph 4=1      # 0              1            2            3          4
g_node_synsets = ['person.n.01', 'wear.v.01', 'coat.n.01', 'be.v.01', 'white.a.01']
g_node_features_words = ['person', 'wears', 'coat', 'is', 'white']
g_node_labels_words = ['doctor', 'none', 'none', 'none', 'none']
g_edge_index = [
    [0, 1, 2, 3],
    [1, 2, 3, 4],
]
init_classes(g_node_synsets)
extend_wn_neighbors(g_node_synsets, g_edge_index)
g_edge_index = torch.tensor(g_edge_index, dtype=torch.long)
append_graph(g_node_synsets, g_node_features_words, g_node_labels_words, g_edge_index)

print(node_features_list[0].shape, node_features_list[0].dtype)
print(node_labels_list[0].shape, node_labels_list[0].dtype)
print(edge_index_list[0].shape, edge_index_list[0].dtype)

PPI_NUM_INPUT_FEATURES = 50
PPI_NUM_CLASSES = len(synsets_to_classes)

synsets_to_classes, classes_to_synsets

def hide_node(g_node_features, g_edge_index, g_node_labels, hide_node_idx=None):
    # TODO: hide percentage of nodes
    node_count = g_node_features.shape[0]
    if hide_node_idx is None:
        hide_node_idx = torch.randint(node_count, 1).item()
    # replace features with random normal
    g_node_features[hide_node_idx] = torch.normal(
        mean=torch.full_like(g_node_features[hide_node_idx], 0.0),
        std=torch.full_like(g_node_features[hide_node_idx], 1.0)
    )
    node_label = g_node_labels[hide_node_idx]
    return g_node_features, g_edge_index, node_label

# edge_index_list = []
# node_features_list = []
# node_labels_list = []

# node_synsets_list = []
# synsets_to_classes = {}
# classes_to_synsets = []
hidden_node_idx_list = list(map(torch.tensor, [6, 0, 2, 2]))
hidden_node_labels_list = []

for gi in range(len(node_labels_list)):
    g_node_features = node_features_list[gi]
    g_edge_index = edge_index_list[gi]
    g_node_labels = node_labels_list[gi]
    hide_node_idx = hidden_node_idx_list[gi]

    g_node_features, g_edge_index, node_label = hide_node(
        g_node_features, g_edge_index, g_node_labels,
        hide_node_idx=hide_node_idx
    )

    node_features_list[gi] = g_node_features
    edge_index_list[gi] = g_edge_index
    hidden_node_labels_list.append(node_label)

hidden_node_labels_list

hidden_node_idx_list

# torch.Size([3021, 50]) torch.float32
# torch.Size([3021, 121]) torch.float32
# torch.Size([2, 94359]) torch.int64
#print(node_labels[0])
#print()
#print(g_node_labels)

# load my dataset
def load_graph_data(training_config, device):
    dataset_name = training_config['dataset_name'].lower()
    should_visualize = training_config['should_visualize']

    # Collect train/val/test graphs here
    #edge_index_list = []
    #node_features_list = []
    #node_labels_list = []
    num_graphs_per_split_cumulative = [0, 1, 2, 3]

    #
    # Prepare graph data loaders
    #

    # Optimization, do a shortcut in case we only need the test data loader
    if training_config['ppi_load_test_only']:
        data_loader_test = GraphDataLoader(
            node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            hidden_node_idx_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            batch_size=training_config['batch_size'],
            shuffle=False
        )
        return data_loader_test
    else:

        num_graphs_per_split_cumulative = [0, -1, -1, -1]  # all
        data_loader_train = GraphDataLoader(
            node_features_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            node_labels_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            edge_index_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            hidden_node_idx_list[num_graphs_per_split_cumulative[0]:num_graphs_per_split_cumulative[1]],
            batch_size=training_config['batch_size'],
            shuffle=True
        )

        num_graphs_per_split_cumulative = [0, 0, -1, -1]  # all
        data_loader_val = GraphDataLoader(
            node_features_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
            node_labels_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
            edge_index_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
            hidden_node_idx_list[num_graphs_per_split_cumulative[1]:num_graphs_per_split_cumulative[2]],
            batch_size=training_config['batch_size'],
            shuffle=False  # no need to shuffle the validation and test graphs
        )

        num_graphs_per_split_cumulative = [0, 0, 0, -1]  # all
        data_loader_test = GraphDataLoader(
            node_features_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
            node_labels_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
            edge_index_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
            hidden_node_idx_list[num_graphs_per_split_cumulative[2]:num_graphs_per_split_cumulative[3]],
            batch_size=training_config['batch_size'],
            shuffle=False
        )

        return data_loader_train, data_loader_val, data_loader_test

"""Nice, there are is this `GraphDataLoader` object that we still haven't defined. We need it in order to load batches of PPI graphs into GAT.

Here we go:
"""

class GraphDataLoader(DataLoader):
    """
    When dealing with batches it's always a good idea to inherit from PyTorch's provided classes (Dataset/DataLoader).

    """
    def __init__(self, node_features_list, node_labels_list, edge_index_list, hidden_node_idx_list, batch_size=1, shuffle=False):
        graph_dataset = GraphDataset(node_features_list, node_labels_list, edge_index_list, hidden_node_idx_list)
        #super().__init__(graph_dataset, batch_size, shuffle)
        # We need to specify a custom collate function, it doesn't work with the default one
        super().__init__(graph_dataset, batch_size, shuffle, collate_fn=graph_collate_fn)


class GraphDataset(Dataset):
    """
    This one just fetches a single graph from the split when GraphDataLoader "asks" it

    """
    def __init__(self, node_features_list, node_labels_list, edge_index_list, hidden_node_idx_list):
        self.node_features_list = node_features_list
        self.node_labels_list = node_labels_list
        self.edge_index_list = edge_index_list
        self.hidden_node_idx_list = hidden_node_idx_list

    # 2 interface functions that need to be defined are len and getitem so that DataLoader can do it's magic
    def __len__(self):
        return len(self.edge_index_list)

    def __getitem__(self, idx):  # we just fetch a single graph
        return self.node_features_list[idx], self.node_labels_list[idx], self.edge_index_list[idx], self.hidden_node_idx_list[idx]


def graph_collate_fn(batch):
    """
    The main idea here is to take multiple graphs from PPI as defined by the batch size
    and merge them into a single graph with multiple connected components.

    It's important to adjust the node ids in edge indices such that they form a consecutive range. Otherwise
    the scatter functions in the implementation 3 will fail.

    :param batch: contains a list of edge_index, node_features, node_labels tuples (as provided by the GraphDataset)
    """

    edge_index_list = []
    node_features_list = []
    node_labels_list = []
    hidden_node_idx_list = []
    num_nodes_seen = 0

    for features_labels_edge_index_tuple in batch:
        # Just collect these into separate lists
        node_features_list.append(features_labels_edge_index_tuple[0])
        node_labels_list.append(features_labels_edge_index_tuple[1])

        edge_index = features_labels_edge_index_tuple[2]  # all of the components are in the [0, N] range
        hidden_node_idx_list.append(features_labels_edge_index_tuple[3])
        edge_index_list.append(edge_index + num_nodes_seen)  # very important! translate the range of this component
        num_nodes_seen += len(features_labels_edge_index_tuple[1])  # update the number of nodes we've seen so far

    # Merge the PPI graphs into a single graph with multiple connected components
    node_features = torch.cat(node_features_list, 0)
    node_labels = torch.cat(node_labels_list, 0)
    edge_index = torch.cat(edge_index_list, 1)
    hidden_node_idx = torch.stack(hidden_node_idx_list)

    return node_features, node_labels, edge_index, hidden_node_idx

torch.stack([torch.tensor(1), torch.tensor(1)])

"""The idea is simple üí° (as all things should be üòú). In order to pass a batch of graphs into GAT we do the following:

1. During the preprocessing step, we map the edge index from the original range into the [0, N] range, where N is the number of nodes in a given graph. Example: the third graph in the training split may contain nodes [3500, ..., 5000] and thus originally its edge index would be in that very same range, e.g.: [3500, 4232], [3808, 4232], ...]. By subtracting the min element, 3500 in this case, we bring the edge index into [0, 1500] range.

---

2. In the `graph_collate_fn` function, edge indices are in the normalized range i.e. [0, N]. What we do is we shift them such that we end up with a single graph that actually consists out of multiple smaller PPI graphs which are not connected with each other i.e. they represent separate [connected components](https://www.geeksforgeeks.org/connected-components-in-an-undirected-graph/). 

---

**Example:** let's say we have 3 graphs in a batch and all 3 edge indices are in the [0, 1000] range. What we'll do is the following: we'll leave the first graph without any modification, we'll shift second graph's range to [1000, 2000], because we had 1000 nodes in the graph that came before, and we'll shift the third graph's range into [2000, 3000], again because we had 2000 nodes that came before this graph. So the final edge index will have nodes in the [0, 3000] range and we treat that as a single graph with multiple connected components. ü§ì

Nice, finally let's try and load it. We should also analyze the shapes - that's always a good idea.
"""

# Let's just define dummy visualization functions for now - just to stop Python interpreter from complaining!
# We'll define them in a moment, properly, I swear.

def plot_in_out_degree_distributions():
    pass

def visualize_graph():
    pass

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # checking whether you have a GPU

config = {
    'dataset_name': DatasetType.PPI.name,
    'should_visualize': False,
    'batch_size': 1,
    'ppi_load_test_only': False  # small optimization for loading test graphs only, we won't use it here
}

data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)
# Let's fetch a single batch from the train graph data loader
node_features, node_labels, edge_index, hidden_node_idx = next(iter(data_loader_train))

print('*' * 20)
print(node_features.shape, node_features.dtype)
print(node_labels.shape, node_labels.dtype)
print(edge_index.shape, edge_index.dtype)
print(hidden_node_idx.shape, hidden_node_idx.dtype)

"""Nice! Analyzing the shapes we see the following:
1. This specific PPI train graph (batch size = 1) has 3021 nodes 
2. Each node has **50 features** (check out [data_loading.py](https://github.com/gordicaleksa/pytorch-GAT/blob/main/utils/data_loading.py) for much more detail)
3. PPI has **121 classes** and each node can have multiple classes associated with it (multi-label classification dataset)
4. This graph has 94359 edges (including the self edges)! (Compare this to 13k edges in Cora)
5. PPI has **20 train** graphs, **2 validation** graphs and **2 test** graphs

Additionally the edge index is of `int 64` type. Why? Well it's a constraint that PyTorch is imposing upon us. `index_select` functions require torch.long (i.e. 64 bit integer) and we use those in GAT implementation 3 - that's it.

node_labels can be `float32` because `nn.BCEWithLogitsLoss` doesn't require long/int64 type (compare that to `nn.CrossEntropyLoss`, that we used in the Cora notebook, which does require int64s). So we can save up 2x memory! Not bad.

---

On the "side note", it's always a **good idea to test your code as you're progressing with your project.** 

Data loading is completely orthogonal to the rest of this notebook so we can test it, standalone, and make sure the shapes and datatypes make sense. I use this strategy while developing projects like this one (and in general).

I start with data, I add the loading functionality, I add some visualizations and only then do I usually start developing the deep learning model itself.

Visualizations are a huge bonus, so let's develop them.

# Visualizing your data üîÆüëì

Let's start by understanding the degree distribution of nodes in PPI - i.e. how many input/output edges do nodes have, a certain measure of connectedness of the graph.

Run the following cell:
"""

def plot_in_out_degree_distributions(edge_index, num_of_nodes, dataset_name):
    """
        Note: It would be easy to do various kinds of powerful network analysis using igraph/networkx, etc.
        I chose to explicitly calculate only the node degree statistics here, but you can go much further if needed and
        calculate the graph diameter, number of triangles and many other concepts from the network analysis field.

    """
    if isinstance(edge_index, torch.Tensor):
        edge_index = edge_index.cpu().numpy()
        
    assert isinstance(edge_index, np.ndarray), f'Expected NumPy array got {type(edge_index)}.'

    # Store each node's input and output degree (they're the same for undirected graphs such as Cora/PPI)
    in_degrees = np.zeros(num_of_nodes, dtype=np.int)
    out_degrees = np.zeros(num_of_nodes, dtype=np.int)

    # Edge index shape = (2, E), the first row contains the source nodes, the second one target/sink nodes
    # Note on terminology: source nodes point to target/sink nodes
    num_of_edges = edge_index.shape[1]
    for cnt in range(num_of_edges):
        source_node_id = edge_index[0, cnt]
        target_node_id = edge_index[1, cnt]

        out_degrees[source_node_id] += 1  # source node points towards some other node -> increment it's out degree
        in_degrees[target_node_id] += 1  # similarly here

    hist = np.zeros(np.max(out_degrees) + 1)
    for out_degree in out_degrees:
        hist[out_degree] += 1

    fig = plt.figure(figsize=(12,8), dpi=100)  # otherwise plots are really small in Jupyter Notebook
    fig.subplots_adjust(hspace=0.6)

    plt.subplot(311)
    plt.plot(in_degrees, color='red')
    plt.xlabel('node id'); plt.ylabel('in-degree count'); plt.title('Input degree for different node ids')

    plt.subplot(312)
    plt.plot(out_degrees, color='green')
    plt.xlabel('node id'); plt.ylabel('out-degree count'); plt.title('Out degree for different node ids')

    plt.subplot(313)
    plt.plot(hist, color='blue')
    plt.xlabel('node degree'); plt.ylabel('# nodes for a given out-degree'); plt.title(f'Node out-degree distribution for {dataset_name} dataset')
    plt.xticks(np.arange(0, len(hist), 20.0))

    plt.grid(True)
    plt.show()

"""Brilliant, let's now visualize PPI's degree distributions!"""

num_of_nodes = len(node_labels)
#plot_in_out_degree_distributions(edge_index, num_of_nodes, config['dataset_name'])

"""You can immediately notice a couple of things:
* The top 2 plots are the same, because we treat PPI as an undirected graph
* Many more nodes have a large number of edges (compared to Cora), but still, most nodes have far less edges
* The third plot nicely visualizes this in the form of a histogram - most nodes have only `1-20` edges (hence the peak on the leftmost side). But it's **more spread out compared to Cora.**

Ok, we're starting to get some valuable insight into PPI, let's continue further and literally visualize/see PPI's graphs.

The following cell will plot the training graph we fetched in one of the previous cells, run it. (*whispers: run it*)
"""

"""
Check out this blog for available graph visualization tools:
    https://towardsdatascience.com/large-graph-visualization-tools-and-approaches-2b8758a1cd59

Basically depending on how big your graph is there may be better drawing tools than igraph.

Note: I unfortunatelly had to flatten this function since igraph is having some problems with Jupyter Notebook,
we'll only call it here so it's fine!

"""
# fsdf
"""
dataset_name = config['dataset_name']
visualization_tool=GraphVisualizationTool.IGRAPH

if isinstance(edge_index, torch.Tensor):
    edge_index_np = edge_index.cpu().numpy()

if isinstance(node_labels, torch.Tensor):
    node_labels_np = node_labels.cpu().numpy()

num_of_nodes = len(node_labels_np)
edge_index_tuples = list(zip(edge_index_np[0, :], edge_index_np[1, :]))  # igraph requires this format

# Construct the igraph graph
ig_graph = ig.Graph()
ig_graph.add_vertices(num_of_nodes)
ig_graph.add_edges(edge_index_tuples)

# Prepare the visualization settings dictionary
visual_style = {}

# Defines the size of the plot and margins
visual_style["bbox"] = (650, 650)
visual_style["margin"] = 5

# I've chosen the edge thickness such that it's proportional to the number of shortest paths (geodesics)
# that go through a certain edge in our graph (edge_betweenness function, a simple ad hoc heuristic)

# line1: I use log otherwise some edges will be too thick and others not visible at all
# edge_betweeness returns < 1 for certain edges that's why I use clip as log would be negative for those edges
# line2: Normalize so that the thickest edge is 1 otherwise edges appear too thick on the chart
# line3: The idea here is to make the strongest edge stay stronger than others, 6 just worked, don't dwell on it

edge_weights_raw = np.clip(np.log(np.asarray(ig_graph.edge_betweenness())+1e-16), a_min=0, a_max=None)
edge_weights_raw_normalized = edge_weights_raw / np.max(edge_weights_raw)
edge_weights = [w**6 for w in edge_weights_raw_normalized]
visual_style["edge_width"] = edge_weights

# A simple heuristic for vertex size.
visual_style["vertex_size"] = [deg / 10 for deg in ig_graph.degree()]

# Set the layout - the way the graph is presented on a 2D chart. Graph drawing is a subfield for itself!
# I used "Kamada Kawai" a force-directed method, this family of methods are based on physical system simulation.
visual_style["layout"] = ig_graph.layout_kamada_kawai()

print('Plotting results ... (it may take couple of seconds).')
ig.plot(ig_graph, **visual_style)
"""

# This website has got some awesome visualizations check it out:
# http://networkrepository.com/graphvis.php?d=./data/gsm50/labeled/cora.edges

"""<img src="data/readme_pics/ppi_graph_jupyter.PNG" alt="PPI visualized" align="center"/> <br/>

**Note: I had to clear the original output of this cell when checking this notebook in, otherwise the file is huuge!** <br/>
**I'm just showing you one arbitrary PPI training graph example here, yours may be different (there are 20 training graphs).**

And I don't know about you, but I think this one is also beautiful! And it's **very different from Cora.** We can see much more edges.

There are also multiple big nodes of similar size whereas in Cora a single node dominates the plot.

Ok, we're done with visualizations and understanding our data. This is a huge milestone, so tap yourself on the back. üèÜüéÇüéµ

We have the level 2 unlocked (the GAT model ü¶Ñ). üòç

And now, let's understand the model! (just **skip the Part 2** and run the cells blindly **if you already went through the 1st part** of this GNN notebook series)

# Part 2: Understanding GAT's inner workings üíªü¶Ñ

First let's create a high level class where we'll build up `GAT` from `GatLayer` objects. 

It basically just stacks the layers into a nn.Sequential object and additionally since nn.Sequential expects a single input (and it has a single output) I just pack the data (features, edge index) into a tuple - *pure syntactic sugar*.
"""

import torch.nn as nn
from torch.optim import Adam


class GAT(torch.nn.Module):
    """
    The most interesting and hardest implementation is implementation #3.
    Imp1 and imp2 differ in subtle details but are basically the same thing.

    So I'll focus on imp #3 in this notebook.

    """

    def __init__(self, num_of_layers, num_heads_per_layer, num_features_per_layer, add_skip_connection=True, bias=True,
                 dropout=0.6, log_attention_weights=False):
        super().__init__()
        assert num_of_layers == len(num_heads_per_layer) == len(num_features_per_layer) - 1, f'Enter valid arch params.'

        num_heads_per_layer = [1] + num_heads_per_layer  # trick - so that I can nicely create GAT layers below

        gat_layers = []  # collect GAT layers
        for i in range(num_of_layers):
            layer = GATLayer(
                num_in_features=num_features_per_layer[i] * num_heads_per_layer[i],  # consequence of concatenation
                num_out_features=num_features_per_layer[i+1],
                num_of_heads=num_heads_per_layer[i+1],
                concat=True if i < num_of_layers - 1 else False,  # last GAT layer does mean avg, the others do concat
                activation=nn.ELU() if i < num_of_layers - 1 else None,  # last layer just outputs raw scores
                dropout_prob=dropout,
                add_skip_connection=add_skip_connection,
                bias=bias,
                log_attention_weights=log_attention_weights
            )
            gat_layers.append(layer)

        self.gat_net = nn.Sequential(
            *gat_layers,
        )

    # data is just a (in_nodes_features, edge_index) tuple, I had to do it like this because of the nn.Sequential:
    # https://discuss.pytorch.org/t/forward-takes-2-positional-arguments-but-3-were-given-for-nn-sqeuential-with-linear-layers/65698
    def forward(self, data):
        return self.gat_net(data)

"""Now for the fun part let's define the layer. 

I really don't think that I can explain it any better, using words, than you taking your time to digest the code and the comments.

Also make sure to check out [my video on GAT](https://www.youtube.com/watch?v=uFLeKkXWq2c) before you start losing time trying to figure it out "from scratch". It's always good to have some theoretical background at your hand.
"""

class GATLayer(torch.nn.Module):
    """
    Implementation #3 was inspired by PyTorch Geometric: https://github.com/rusty1s/pytorch_geometric

    But, it's hopefully much more readable! (and of similar performance)

    """
    
    # We'll use these constants in many functions so just extracting them here as member fields
    src_nodes_dim = 0  # position of source nodes in edge index
    trg_nodes_dim = 1  # position of target nodes in edge index

    # These may change in the inductive setting - leaving it like this for now (not future proof)
    nodes_dim = 0      # node dimension (axis is maybe a more familiar term nodes_dim is the position of "N" in tensor)
    head_dim = 1       # attention head dim

    def __init__(self, num_in_features, num_out_features, num_of_heads, concat=True, activation=nn.ELU(),
                 dropout_prob=0.6, add_skip_connection=True, bias=True, log_attention_weights=False):

        super().__init__()

        self.num_of_heads = num_of_heads
        self.num_out_features = num_out_features
        self.concat = concat  # whether we should concatenate or average the attention heads
        self.add_skip_connection = add_skip_connection

        #
        # Trainable weights: linear projection matrix (denoted as "W" in the paper), attention target/source
        # (denoted as "a" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)
        #

        # You can treat this one matrix as num_of_heads independent W matrices
        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)

        # After we concatenate target node (node i) and source node (node j) we apply the "additive" scoring function
        # which gives us un-normalized score "e". Here we split the "a" vector - but the semantics remain the same.
        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with "a"
        # we instead do a dot product between x and "a_left" and y and "a_right" and we sum them up
        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))
        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))

        # Bias is definitely not crucial to GAT - feel free to experiment (I pinged the main author, Petar, on this one)
        if bias and concat:
            self.bias = nn.Parameter(torch.Tensor(num_of_heads * num_out_features))
        elif bias and not concat:
            self.bias = nn.Parameter(torch.Tensor(num_out_features))
        else:
            self.register_parameter('bias', None)

        if add_skip_connection:
            self.skip_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)
        else:
            self.register_parameter('skip_proj', None)

        #
        # End of trainable weights
        #

        self.leakyReLU = nn.LeakyReLU(0.2)  # using 0.2 as in the paper, no need to expose every setting
        self.activation = activation
        # Probably not the nicest design but I use the same module in 3 locations, before/after features projection
        # and for attention coefficients. Functionality-wise it's the same as using independent modules.
        self.dropout = nn.Dropout(p=dropout_prob)

        self.log_attention_weights = log_attention_weights  # whether we should log the attention weights
        self.attention_weights = None  # for later visualization purposes, I cache the weights here

        self.init_params()
        
    def forward(self, data):
        #
        # Step 1: Linear Projection + regularization
        #

        in_nodes_features, edge_index = data  # unpack data
        num_of_nodes = in_nodes_features.shape[self.nodes_dim]
        assert edge_index.shape[0] == 2, f'Expected edge index with shape=(2,E) got {edge_index.shape}'

        # shape = (N, FIN) where N - number of nodes in the graph, FIN - number of input features per node
        # We apply the dropout to all of the input node features (as mentioned in the paper)
        in_nodes_features = self.dropout(in_nodes_features)

        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features
        # We project the input node features into NH independent output features (one for each attention head)
        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)

        nodes_features_proj = self.dropout(nodes_features_proj)  # in the official GAT imp they did dropout here as well

        #
        # Step 2: Edge attention calculation
        #

        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)
        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension
        # Optimization note: torch.sum() is as performant as .sum() in my experiments
        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)
        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)

        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all
        # the possible combinations of scores we just prepare those that will actually be used and those are defined
        # by the edge index.
        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph
        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)
        scores_per_edge = self.leakyReLU(scores_source_lifted + scores_target_lifted)

        # shape = (E, NH, 1)
        attentions_per_edge = self.neighborhood_aware_softmax(scores_per_edge, edge_index[self.trg_nodes_dim], num_of_nodes)
        # Add stochasticity to neighborhood aggregation
        attentions_per_edge = self.dropout(attentions_per_edge)

        #
        # Step 3: Neighborhood aggregation
        #

        # Element-wise (aka Hadamard) product. Operator * does the same thing as torch.mul
        # shape = (E, NH, FOUT) * (E, NH, 1) -> (E, NH, FOUT), 1 gets broadcast into FOUT
        nodes_features_proj_lifted_weighted = nodes_features_proj_lifted * attentions_per_edge

        # This part sums up weighted and projected neighborhood feature vectors for every target node
        # shape = (N, NH, FOUT)
        out_nodes_features = self.aggregate_neighbors(nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes)

        #
        # Step 4: Residual/skip connections, concat and bias
        #

        out_nodes_features = self.skip_concat_bias(attentions_per_edge, in_nodes_features, out_nodes_features)
        return (out_nodes_features, edge_index)

    #
    # Helper functions (without comments there is very little code so don't be scared!)
    #

    def neighborhood_aware_softmax(self, scores_per_edge, trg_index, num_of_nodes):
        """
        As the fn name suggest it does softmax over the neighborhoods. Example: say we have 5 nodes in a graph.
        Two of them 1, 2 are connected to node 3. If we want to calculate the representation for node 3 we should take
        into account feature vectors of 1, 2 and 3 itself. Since we have scores for edges 1-3, 2-3 and 3-3
        in scores_per_edge variable, this function will calculate attention scores like this: 1-3/(1-3+2-3+3-3)
        (where 1-3 is overloaded notation it represents the edge 1-3 and its (exp) score) and similarly for 2-3 and 3-3
         i.e. for this neighborhood we don't care about other edge scores that include nodes 4 and 5.

        Note:
        Subtracting the max value from logits doesn't change the end result but it improves the numerical stability
        and it's a fairly common "trick" used in pretty much every deep learning framework.
        Check out this link for more details:

        https://stats.stackexchange.com/questions/338285/how-does-the-subtraction-of-the-logit-maximum-improve-learning

        """
        # Calculate the numerator. Make logits <= 0 so that e^logit <= 1 (this will improve the numerical stability)
        scores_per_edge = scores_per_edge - scores_per_edge.max()
        exp_scores_per_edge = scores_per_edge.exp()  # softmax

        # Calculate the denominator. shape = (E, NH)
        neigborhood_aware_denominator = self.sum_edge_scores_neighborhood_aware(exp_scores_per_edge, trg_index, num_of_nodes)

        # 1e-16 is theoretically not needed but is only there for numerical stability (avoid div by 0) - due to the
        # possibility of the computer rounding a very small number all the way to 0.
        attentions_per_edge = exp_scores_per_edge / (neigborhood_aware_denominator + 1e-16)

        # shape = (E, NH) -> (E, NH, 1) so that we can do element-wise multiplication with projected node features
        return attentions_per_edge.unsqueeze(-1)

    def sum_edge_scores_neighborhood_aware(self, exp_scores_per_edge, trg_index, num_of_nodes):
        # The shape must be the same as in exp_scores_per_edge (required by scatter_add_) i.e. from E -> (E, NH)
        trg_index_broadcasted = self.explicit_broadcast(trg_index, exp_scores_per_edge)

        # shape = (N, NH), where N is the number of nodes and NH the number of attention heads
        size = list(exp_scores_per_edge.shape)  # convert to list otherwise assignment is not possible
        size[self.nodes_dim] = num_of_nodes
        neighborhood_sums = torch.zeros(size, dtype=exp_scores_per_edge.dtype, device=exp_scores_per_edge.device)

        # position i will contain a sum of exp scores of all the nodes that point to the node i (as dictated by the
        # target index)
        neighborhood_sums.scatter_add_(self.nodes_dim, trg_index_broadcasted, exp_scores_per_edge)

        # Expand again so that we can use it as a softmax denominator. e.g. node i's sum will be copied to
        # all the locations where the source nodes pointed to i (as dictated by the target index)
        # shape = (N, NH) -> (E, NH)
        return neighborhood_sums.index_select(self.nodes_dim, trg_index)

    def aggregate_neighbors(self, nodes_features_proj_lifted_weighted, edge_index, in_nodes_features, num_of_nodes):
        size = list(nodes_features_proj_lifted_weighted.shape)  # convert to list otherwise assignment is not possible
        size[self.nodes_dim] = num_of_nodes  # shape = (N, NH, FOUT)
        out_nodes_features = torch.zeros(size, dtype=in_nodes_features.dtype, device=in_nodes_features.device)

        # shape = (E) -> (E, NH, FOUT)
        trg_index_broadcasted = self.explicit_broadcast(edge_index[self.trg_nodes_dim], nodes_features_proj_lifted_weighted)
        # aggregation step - we accumulate projected, weighted node features for all the attention heads
        # shape = (E, NH, FOUT) -> (N, NH, FOUT)
        out_nodes_features.scatter_add_(self.nodes_dim, trg_index_broadcasted, nodes_features_proj_lifted_weighted)

        return out_nodes_features

    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):
        """
        Lifts i.e. duplicates certain vectors depending on the edge index.
        One of the tensor dims goes from N -> E (that's where the "lift" comes from).

        """
        src_nodes_index = edge_index[self.src_nodes_dim]
        trg_nodes_index = edge_index[self.trg_nodes_dim]

        # Using index_select is faster than "normal" indexing (scores_source[src_nodes_index]) in PyTorch!
        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)
        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)
        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)

        return scores_source, scores_target, nodes_features_matrix_proj_lifted

    def explicit_broadcast(self, this, other):
        # Append singleton dimensions until this.dim() == other.dim()
        for _ in range(this.dim(), other.dim()):
            this = this.unsqueeze(-1)

        # Explicitly expand so that shapes are the same
        return this.expand_as(other)

    def init_params(self):
        """
        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:
            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow

        The original repo was developed in TensorFlow (TF) and they used the default initialization.
        Feel free to experiment - there may be better initializations depending on your problem.

        """
        nn.init.xavier_uniform_(self.linear_proj.weight)
        nn.init.xavier_uniform_(self.scoring_fn_target)
        nn.init.xavier_uniform_(self.scoring_fn_source)

        if self.bias is not None:
            torch.nn.init.zeros_(self.bias)

    def skip_concat_bias(self, attention_coefficients, in_nodes_features, out_nodes_features):
        if self.log_attention_weights:  # potentially log for later visualization in playground.py
            self.attention_weights = attention_coefficients

        if self.add_skip_connection:  # add skip or residual connection
            if out_nodes_features.shape[-1] == in_nodes_features.shape[-1]:  # if FIN == FOUT
                # unsqueeze does this: (N, FIN) -> (N, 1, FIN), out features are (N, NH, FOUT) so 1 gets broadcast to NH
                # thus we're basically copying input vectors NH times and adding to processed vectors
                out_nodes_features += in_nodes_features.unsqueeze(1)
            else:
                # FIN != FOUT so we need to project input feature vectors into dimension that can be added to output
                # feature vectors. skip_proj adds lots of additional capacity which may cause overfitting.
                out_nodes_features += self.skip_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)

        if self.concat:
            # shape = (N, NH, FOUT) -> (N, NH*FOUT)
            out_nodes_features = out_nodes_features.view(-1, self.num_of_heads * self.num_out_features)
        else:
            # shape = (N, NH, FOUT) -> (N, FOUT)
            out_nodes_features = out_nodes_features.mean(dim=self.head_dim)

        if self.bias is not None:
            out_nodes_features += self.bias

        return out_nodes_features if self.activation is None else self.activation(out_nodes_features)

"""The main idea that leads to huge savings is that we calculate the scores only for the nodes that will actually be used and not for every imaginable combination (that would be valid only in a fully-connected graph).

Once we compute the `"left"` scores and the `"right"` scores, we "lift" them up using the edge index. That way
if the edge `1->2` is not present in the graph we won't have those score pairs in our data structure.

After adding lifted "left" and "right" (or maybe a better naming would be source and target) scores we do smart `neighborhood-aware softmax` - so that the semantics of GAT is respected. After doing the `scatter add` (which you should take your time to understand and go through the docs) we can combine the projected feature vectors, and voil√†, we got ourselves a fully-blown GAT layer.

---

Take your time and **be patient**! Especially if you're new to GNNs. 

I didn't learn all of this in 1 day, it takes time for the knowledge to sink in. You'll get there as well! ‚ù§Ô∏è (if you're not already there üòú)

Having said that, we've got the level 3 unlocked (model training üí™). üòç

We have the data üìú ready, we have the GAT model ü¶Ñ ready, let's start training this beast! üí™

# Part 3: Training GAT üí™ (Multi-label classification on PPI!)

Phew, well the hardest part is behind us. Let's know create a simple training loop where the goal is to learn to assign multiple labels to PPI nodes.

But first let's define some relevant constants.
"""

from torch.utils.tensorboard import SummaryWriter


# 3 different model training/eval phases used in train.py
class LoopPhase(enum.Enum):
    TRAIN = 0,
    VAL = 1,
    TEST = 2

    
writer = SummaryWriter()  # (tensorboard) writer will output to ./runs/ directory by default


# Global vars used for early stopping. After some number of epochs (as defined by the patience_period var) without any
# improvement on the validation dataset (measured via micro-F1 metric), we'll break out from the training loop.
BEST_VAL_MICRO_F1 = 0
BEST_VAL_LOSS = 0
PATIENCE_CNT = 0

BINARIES_PATH = os.path.join(os.getcwd(), 'models', 'binaries')
CHECKPOINTS_PATH = os.path.join(os.getcwd(), 'models', 'checkpoints')

# Make sure these exist as the rest of the code assumes it
os.makedirs(BINARIES_PATH, exist_ok=True)
os.makedirs(CHECKPOINTS_PATH, exist_ok=True)

"""Also, let's define a couple of functions that will be useful while training the model.

The training state contains a lot of useful `metadata` which we can later use. You can imagine that saving the test accuracy of your model is important, especially when you're training your models on a cloud - it makes the organization so much better.
"""

import git
import re  # regex


def get_training_state(training_config, model):
    training_state = {
        "commit_hash": git.Repo(search_parent_directories=True).head.object.hexsha,

        # Training details
        "dataset_name": training_config['dataset_name'],
        "num_of_epochs": training_config['num_of_epochs'],
        "test_perf": training_config['test_perf'],

        # Model structure
        "num_of_layers": training_config['num_of_layers'],
        "num_heads_per_layer": training_config['num_heads_per_layer'],
        "num_features_per_layer": training_config['num_features_per_layer'],
        "add_skip_connection": training_config['add_skip_connection'],
        "bias": training_config['bias'],
        "dropout": training_config['dropout'],

        # Model state
        "state_dict": model.state_dict()
    }

    return training_state


def print_model_metadata(training_state):
    header = f'\n{"*"*5} Model training metadata: {"*"*5}'
    print(header)

    for key, value in training_state.items():
        if key != 'state_dict':  # don't print state_dict it's a bunch of numbers...
            print(f'{key}: {value}')
    print(f'{"*" * len(header)}\n')
    

# This one makes sure we don't overwrite the valuable model binaries (feel free to ignore - not crucial to GAT method)
def get_available_binary_name(dataset_name='unknown'):
    prefix = f'gat_{dataset_name}'

    def valid_binary_name(binary_name):
        # First time you see raw f-string? Don't worry the only trick is to double the brackets.
        pattern = re.compile(rf'{prefix}_[0-9]{{6}}\.pth')
        return re.fullmatch(pattern, binary_name) is not None

    # Just list the existing binaries so that we don't overwrite them but write to a new one
    valid_binary_names = list(filter(valid_binary_name, os.listdir(BINARIES_PATH)))
    if len(valid_binary_names) > 0:
        last_binary_name = sorted(valid_binary_names)[-1]
        new_suffix = int(last_binary_name.split('.')[0][-6:]) + 1  # increment by 1
        return f'{prefix}_{str(new_suffix).zfill(6)}.pth'
    else:
        return f'{prefix}_000000.pth'

"""Nice, now `argparse` is just a nice way to **organize** your program settings:"""

PPI_NUM_INPUT_FEATURES, PPI_NUM_CLASSES

import argparse


def get_training_args():
    parser = argparse.ArgumentParser()

    # Training related
    parser.add_argument("--num_of_epochs", type=int, help="number of training epochs", default=200)
    parser.add_argument("--patience_period", type=int, help="number of epochs with no improvement on val before terminating", default=100)
    parser.add_argument("--lr", type=float, help="model learning rate", default=5e-3)
    parser.add_argument("--weight_decay", type=float, help="L2 regularization on model weights", default=0)
    parser.add_argument("--should_test", type=bool, help='should test the model on the test dataset?', default=True)
    parser.add_argument("--force_cpu", type=bool, help='use CPU if your GPU is too small', default=False)

    # Dataset related (note: we need the dataset name for metadata and related stuff, and not for picking the dataset)
    parser.add_argument("--dataset_name", choices=[el.name for el in DatasetType], help='dataset to use for training', default=DatasetType.PPI.name)
    parser.add_argument("--batch_size", type=int, help='number of graphs in a batch', default=1)
    parser.add_argument("--should_visualize", type=bool, help='should visualize the dataset?', default=False)

    # Logging/debugging/checkpoint related (helps a lot with experimentation)
    parser.add_argument("--enable_tensorboard", type=bool, help="enable tensorboard logging", default=False)
    parser.add_argument("--console_log_freq", type=int, help="log to output console (epoch) freq (None for no logging)", default=10)
    parser.add_argument("--checkpoint_freq", type=int, help="checkpoint model saving (epoch) freq (None for no logging)", default=5)
    args = parser.parse_args('')

    # I'm leaving the hyperparam values as reported in the paper, but I experimented a bit and the comments suggest
    # how you can make GAT achieve an even higher micro-F1 or make it smaller
    gat_config = {
        # GNNs, contrary to CNNs, are often shallow (it ultimately depends on the graph properties)
        "num_of_layers": 3,  # PPI has got 42% of nodes with all 0 features - that's why 3 layers are useful
        "num_heads_per_layer": [4, 4, 6],  # other values may give even better results from the reported ones
        "num_features_per_layer": [PPI_NUM_INPUT_FEATURES, 64, 64, PPI_NUM_CLASSES],  # 64 would also give ~0.975 uF1!
        "add_skip_connection": True,  # skip connection is very important! (keep it otherwise micro-F1 is almost 0)
        "bias": True,  # bias doesn't matter that much
        "dropout": 0.0,  # dropout hurts the performance (best to keep it at 0)
    }

    # Wrapping training configuration into a dictionary
    training_config = dict()
    for arg in vars(args):
        training_config[arg] = getattr(args, arg)
    training_config['ppi_load_test_only'] = False  # load both train/val/test data loaders (don't change it)

    # Add additional config information
    training_config.update(gat_config)

    return training_config

"""Now for the juicy part. üç™üéÖ

Here, we organize, high-level, everything we need for training GAT. Just combining the components we already learned.
"""

import time


def train_gat_ppi(config):
    """
    Very similar to Cora's training script. The main differences are:
    1. Using dataloaders since we're dealing with an inductive setting - multiple graphs per batch
    2. Doing multi-class classification (BCEWithLogitsLoss) and reporting micro-F1 instead of accuracy
    3. Model architecture and hyperparams are a bit different (as reported in the GAT paper)

    """
    global BEST_VAL_MICRO_F1, BEST_VAL_LOSS

    # Checking whether you have a strong GPU. Since PPI training requires almost 8 GBs of VRAM
    # I've added the option to force the use of CPU even though you have a GPU on your system (but it's too weak).
    device = torch.device("cuda" if torch.cuda.is_available() and not config['force_cpu'] else "cpu")

    # Step 1: prepare the data loaders
    data_loader_train, data_loader_val, data_loader_test = load_graph_data(config, device)

    # Step 2: prepare the model
    gat = GAT(
        num_of_layers=config['num_of_layers'],
        num_heads_per_layer=config['num_heads_per_layer'],
        num_features_per_layer=config['num_features_per_layer'],
        add_skip_connection=config['add_skip_connection'],
        bias=config['bias'],
        dropout=config['dropout'],
        log_attention_weights=False  # no need to store attentions, used only in playground.py for visualizations
    ).to(device)

    # Step 3: Prepare other training related utilities (loss & optimizer and decorator function)
    #loss_fn = nn.BCEWithLogitsLoss(reduction='mean')
    #weight = torch.tensor([0.25] + ([1] * (PPI_NUM_CLASSES - 1)), dtype=torch.float32).to(device)
    #print('weight =', weight)
    #loss_fn = nn.CrossEntropyLoss(weight=weight, reduction='sum')
    loss_fn = nn.CrossEntropyLoss()
    optimizer = Adam(gat.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])

    # The decorator function makes things cleaner since there is a lot of redundancy between the train and val loops
    main_loop = get_main_loop(
        config,
        gat,
        loss_fn,
        optimizer,
        config['patience_period'],
        time.time())

    BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT = [0, 0, 0]  # reset vars used for early stopping

    # Step 4: Start the training procedure
    for epoch in range(config['num_of_epochs']):
        # Training loop
        main_loop(phase=LoopPhase.TRAIN, data_loader=data_loader_train, epoch=epoch)

        # Validation loop
        with torch.no_grad():
            try:
                main_loop(phase=LoopPhase.VAL, data_loader=data_loader_val, epoch=epoch)
            except Exception as e:  # "patience has run out" exception :O
                print(str(e))
                break  # break out from the training loop

    # Step 5: Potentially test your model
    # Don't overfit to the test dataset - only when you've fine-tuned your model on the validation dataset should you
    # report your final loss and micro-F1 on the test dataset. Friends don't let friends overfit to the test data. <3
    if config['should_test']:
        micro_f1 = main_loop(phase=LoopPhase.TEST, data_loader=data_loader_test)
        config['test_perf'] = micro_f1

        print('*' * 50)
        print(f'Test accuracy = {micro_f1}')
    else:
        config['test_perf'] = -1

    # Save the latest GAT in the binaries directory
    torch.save(
        get_training_state(config, gat),
        os.path.join(BINARIES_PATH, get_available_binary_name(config['dataset_name']))
    )

"""üéâüéâüéâ 

Now for the core part of the training - the main loop, as I've dubbed it. 

I've organized it like this so that I don't have to copy/paste bunch of the same code for train/val/test loops.

**Friends don't let friends copy/paste (unless it's from the Stack Overflow)**
"""

#from sklearn.metrics import f1_score


# Simple decorator function so that I don't have to pass arguments that don't change from epoch to epoch
def get_main_loop(config, gat, sigmoid_cross_entropy_loss, optimizer, patience_period, time_start):

    device = next(gat.parameters()).device  # fetch the device info from the model instead of passing it as a param

    def main_loop(phase, data_loader, epoch=0):
        global BEST_VAL_MICRO_F1, BEST_VAL_LOSS, PATIENCE_CNT, writer

        # Certain modules behave differently depending on whether we're training the model or not.
        # e.g. nn.Dropout - we only want to drop model weights during the training.
        if phase == LoopPhase.TRAIN:
            gat.train()
        else:
            gat.eval()

        # Iterate over batches of graph data (2 graphs per batch was used in the original paper for the PPI dataset)
        # We merge them into a single graph with 2 connected components, that's the main idea. After that
        # the implementation #3 is agnostic to the fact that those are multiple and not a single graph!
        for batch_idx, (node_features, gt_node_labels, edge_index, hidden_node_idx) in enumerate(data_loader):
            # Push the batch onto GPU - note PPI is to big to load the whole dataset into a normal GPU
            # it takes almost 8 GBs of VRAM to train it on a GPU
            edge_index = edge_index.to(device)
            node_features = node_features.to(device)
            gt_node_labels = gt_node_labels.to(device)
            hidden_node_idx = hidden_node_idx.to(device)

            # I pack data into tuples because GAT uses nn.Sequential which expects this format
            graph_data = (node_features, edge_index)

            # Note: [0] just extracts the node_features part of the data (index 1 contains the edge_index)
            # shape = (N, C) where N is the number of nodes in the batch and C is the number of classes (121 for PPI)
            # GAT imp #3 is agnostic to the fact that we actually have multiple graphs
            # (it sees a single graph with multiple connected components)
            nodes_unnormalized_scores = gat(graph_data)[0]

            # Example: because PPI has 121 labels let's make a simple toy example that will show how the loss works.
            # Let's say we have 3 labels instead and a single node's unnormalized (raw GAT output) scores are [-3, 0, 3]
            # What this loss will do is first it will apply a sigmoid and so we'll end up with: [0.048, 0.5, 0.95]
            # next it will apply a binary cross entropy across all of these and find the average, and that's it!
            # So if the true classes were [0, 0, 1] the loss would be (-log(1-0.048) + -log(1-0.5) + -log(0.95))/3.
            # You can see that the logarithm takes 2 forms depending on whether the true label is 0 or 1,
            # either -log(1-x) or -log(x) respectively. Easy-peasy. <3
            #loss = sigmoid_cross_entropy_loss(nodes_unnormalized_scores, gt_node_labels)
            
            # Get label for the hidden node
            #print(hidden_node_idx.shape, hidden_node_idx.dtype)
            loss = sigmoid_cross_entropy_loss(
                nodes_unnormalized_scores,
                gt_node_labels
            )

            if phase == LoopPhase.TRAIN:
                optimizer.zero_grad()  # clean the trainable weights gradients in the computational graph (.grad fields)
                loss.backward()  # compute the gradients for every trainable weight in the computational graph
                optimizer.step()  # apply the gradients to weights

            # Calculate the main metric - micro F1, check out this link for what micro-F1 exactly is:
            # https://www.kaggle.com/enforcer007/what-is-micro-averaged-f1-score

            # Convert unnormalized scores into predictions. Explanation:
            # If the unnormalized score is bigger than 0 that means that sigmoid would have a value higher than 0.5
            # (by sigmoid's definition) and thus we have predicted 1 for that label otherwise we have predicted 0.
            #pred = (nodes_unnormalized_scores > 0).float().cpu().numpy()
            #print(nodes_unnormalized_scores, hidden_node_idx)
            pred = nodes_unnormalized_scores[hidden_node_idx]
            #print(gt_node_labels, hidden_node_idx)
            gt = gt_node_labels[hidden_node_idx]
            #micro_f1 = f1_score(gt, pred, average='micro')
            #print(gt.shape)
            accuracy = torch.sum(torch.argmax(pred, dim=-1) == gt) / gt.shape[0]

            #
            # Logging
            #

            global_step = len(data_loader) * epoch + batch_idx
            if phase == LoopPhase.TRAIN:
                # Log metrics
                if config['enable_tensorboard']:
                    writer.add_scalar('training_loss', loss.item(), global_step)
                    #writer.add_scalar('training_micro_f1', micro_f1, global_step)
                    writer.add_scalar('training_accuracy', accuracy, global_step)

                # Log to console
                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:
                    print(f'GAT training: time elapsed= {(time.time() - time_start):.2f} [s] |'
                          f' epoch={epoch + 1} | batch={batch_idx + 1} | train accuracy={accuracy}.')

                # Save model checkpoint
                if config['checkpoint_freq'] is not None and (epoch + 1) % config['checkpoint_freq'] == 0 and batch_idx == 0:
                    ckpt_model_name = f'gat_{config["dataset_name"]}_ckpt_epoch_{epoch + 1}.pth'
                    config['test_perf'] = -1  # test perf not calculated yet, note: perf means main metric micro-F1 here
                    torch.save(get_training_state(config, gat), os.path.join(CHECKPOINTS_PATH, ckpt_model_name))

            elif phase == LoopPhase.VAL:
                # Log metrics
                if config['enable_tensorboard']:
                    writer.add_scalar('val_loss', loss.item(), global_step)
                    #writer.add_scalar('val_micro_f1', micro_f1, global_step)
                    writer.add_scalar('val_accuracy', accuracy, global_step)

                # Log to console
                if config['console_log_freq'] is not None and epoch % config['console_log_freq'] == 0 and batch_idx == 0:
                    print(f'GAT validation: time elapsed= {(time.time() - time_start):.2f} [s] |'
                          f' epoch={epoch + 1} | batch={batch_idx + 1} | val accuracy={accuracy}')

                # The "patience" logic - should we break out from the training loop? If either validation micro-F1
                # keeps going up or the val loss keeps going down we won't stop
                if accuracy > BEST_VAL_MICRO_F1 or loss.item() < BEST_VAL_LOSS:
                    BEST_VAL_MICRO_F1 = max(accuracy, BEST_VAL_MICRO_F1)  # keep track of the best validation micro_f1 so far
                    BEST_VAL_LOSS = min(loss.item(), BEST_VAL_LOSS)  # and the minimal loss
                    PATIENCE_CNT = 0  # reset the counter every time we encounter new best micro_f1
                else:
                    PATIENCE_CNT += 1  # otherwise keep counting

                if PATIENCE_CNT >= patience_period:
                    raise Exception('Stopping the training, the universe has no more patience for this training.')

            else:
                return accuracy  # in the case of test phase we just report back the test accuracy

    return main_loop  # return the decorated function

"""That was all we needed! Let's train it! üí™üí™üí™

Keep in mind that PPI training takes much more time than training on Cora.

Additionally you'll need 8+ GBs GPU if you want to train it on your GPU. Alternatively you can set the `--force_cpu` flag in the `get_training_args` function to train it on your CPU or simply use the pre-checked-in model I provided. The following section, part 4, doesn't depend on this so you can skip it if you want.
"""

(
torch.tensor([0, 1, 7]).cpu()[torch.tensor([0])],
torch.tensor([0, 1, 7]).cpu().numpy()[torch.tensor([0])],
torch.tensor([0, 1, 7]).cpu()[torch.tensor([0])].numpy(),
torch.tensor([0, 1, 7]).cpu()[torch.tensor([0])].item(),
)

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

tdevice = "cuda"
torch.tensor([[ 0.0204, -0.0142,  0.0315,  0.0839,  0.0737,  0.0080, -0.0036,  0.0037,
          0.0589, -0.0117,  0.0437],
        [ 0.1297, -0.0331,  0.3024,  0.1374,  0.3831,  0.0236, -0.0127, -0.0769,
         -0.1832, -0.0193, -0.0237],
        [-0.1591, -0.1661,  0.2703,  0.1370,  0.2118,  0.1925,  0.0066, -0.0837,
         -0.1723, -0.4315, -0.0919],
        [ 0.0497, -0.0554,  0.3251,  0.1413,  0.2814,  0.0033, -0.0374, -0.0427,
         -0.1156, -0.0459, -0.0155],
        [-0.2426,  0.0937,  0.1353, -0.0122,  0.1045,  0.1565,  0.2281, -0.0732,
         -0.1692, -0.2659, -0.0794],
        [ 0.3328,  0.6199, -0.0306, -0.1698,  0.2248,  0.2094,  0.2737,  0.5563,
          0.1222, -0.0609, -0.1869],
        [ 0.3402,  0.3893,  0.0630,  0.2243,  0.0644,  0.3032,  0.0634,  0.2643,
         -0.4176,  0.1111, -0.1333]]).to(tdevice)[
torch.tensor([6]).to(tdevice)
         ]

# Train the graph attention network (GAT)
train_gat_ppi(get_training_args())

"""Nice!!! üéâüéâüéâ Level 4 unlocked (GAT visualizations üîÆ). üòç

We just achieved `0.978` micro-F1 on PPI's test graphs! The same numbers as reported in the original GAT paper!

So we now have everything in place:
1. PPI data loading and visualizations üìú -> checked
2. GAT model defined ü¶Ñ -> checked
3. Training loop setup and the trained model binaries üí™ -> checked

Now let's play the GAT model under a microscope üî¨üî¨üî¨ and understand the weights we got - we can do that in many ways.

# Part 4: Visualizing GAT on PPI üîÆ

I tried visualizing PPI's 2D embeddings using t-SNE without any label/color information but it's not that informative, so we'll only do **attention** and **entropy visualizations** in this notebook.

Let's start by defining some functions we'll need. 

The following cell's code snippet will get called multiple times so let's just extract it inside a function - a nice modular design.

*Note: the main reason is actually that igraph is having problems with Jupyter so I'm working around it, check out the [original code](https://github.com/gordicaleksa/pytorch-GAT/blob/main/playground.py#L147) if you're curious* üòÇ
"""

def gat_forward_pass(model_name, dataset_name):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # checking whether you have a GPU, I hope so!

    config = {
        'dataset_name': dataset_name,
        'should_visualize': False,  # don't visualize the dataset
        'batch_size': 2,  # we're using 2 graphs per batch as reported in the paper
        'ppi_load_test_only': True  # optimization, we're loading only test graphs
    }

    # Step 1: Prepare the data
    data_loader_test = load_graph_data(config, device)
    node_features, node_labels, topology, hidden_node_idx = next(iter(data_loader_test))
    node_features = node_features.to(device)  # need to explicitly push them to GPU since PPI eats up a lot of VRAM
    node_labels = node_labels.to(device)
    topology = topology.to(device)

    # Step 2: Prepare the model
    model_path = os.path.join(BINARIES_PATH, model_name)
    model_state = torch.load(model_path)

    gat = GAT(
        num_of_layers=model_state['num_of_layers'],
        num_heads_per_layer=model_state['num_heads_per_layer'],
        num_features_per_layer=model_state['num_features_per_layer'],
        add_skip_connection=model_state['add_skip_connection'],
        bias=model_state['bias'],
        dropout=model_state['dropout'],
        log_attention_weights=True
    ).to(device)

    print_model_metadata(model_state)
    assert model_state['dataset_name'].lower() == dataset_name.lower(), \
        f"The model was trained on {model_state['dataset_name']} but you're calling it on {dataset_name}."
    gat.load_state_dict(model_state["state_dict"], strict=True)
    gat.eval()  # some layers like nn.Dropout behave differently in train vs eval mode so this part is important

    # Step 3: Calculate the things we'll need for different visualization types (attention, scores, edge_index)

    # This context manager is important (and you'll often see it), otherwise PyTorch will eat much more memory.
    # It would be saving activations for backprop but we are not going to do any model training just the prediction.
    with torch.no_grad():
        # Step 3: Run predictions and collect the high dimensional data
        all_nodes_unnormalized_scores, _ = gat((node_features, topology))  # shape = (N, num of classes)
        all_nodes_unnormalized_scores = all_nodes_unnormalized_scores.cpu().numpy()

    return all_nodes_unnormalized_scores, topology, node_labels, gat

model_name = 'gat_PPI_000001.pth'
dataset_name=DatasetType.PPI.name
all_nodes_unnormalized_scores, edge_index, node_labels, gat = gat_forward_pass(model_name, dataset_name)

def printnp(a):
    for row in a:
        for col in row:
            print('{: 4.2f}'.format(col), end=' ')
        print('argmax =', np.argmax(row, axis=-1))

#np.set_printoptions(suppress=True)
for node_features, node_labels, edge_index, hidden_node_idx in zip(node_features_list, node_labels_list, edge_index_list, hidden_node_idx_list):
    preds, _ = gat((node_features.cuda(), edge_index.cuda()))
    preds = torch.softmax(preds, -1)
    preds = preds.cpu().detach().numpy()
    node_labels = node_labels.cpu().detach().numpy()
    print('<' * 24)
    printnp(preds)
    print('-' * 24)
    #print(node_labels)
    hidden = np.zeros_like(node_labels, dtype=np.int64)
    hidden[hidden_node_idx] = 1
    print(np.vstack([node_labels, hidden]))
    print('>' * 24)
    #print(preds, node_labels, sep='\n')
#np.set_printoptions(suppress=False)

exit(0)

"""Nice that one just produces the data that'll get consumed in the downstream visualizations that you'll see defined in the following cells.

# Visualizing neighborhood attention üì£

So, you now hopefully understand how GAT roughly works, and so you know that during the aggregation stage every single node assigns an **attention coefficient** to every single one of its neighbors (including itself since we added self edges).

Any ideas on what we could visualize? Well let's pick some nodes and see which attention patterns they've learned!

The first idea that may pop to your mind is to draw edges **thicker** if the **attention is larger** and vice versa (*well that's also the last idea that pops to my mind*).

Let's do it!
"""

# Again, unfortunately, igraph is having some problems running in Jupyter so I have to flatten out the content here
# including the for loops - no for loops with igraph in Jupyter folks.

model_name=r'gat_PPI_000000.pth'  # This model is checked-in, feel free to use the one you trained
dataset_name=DatasetType.PPI.name

# Fetch the data we'll need to create visualizations
all_nodes_unnormalized_scores, edge_index, node_labels, gat = gat_forward_pass(model_name, dataset_name)

# The number of nodes for which we want to visualize their attention over neighboring nodes
# (2x this actually as we add nodes with highest degree + random nodes)
num_nodes_of_interest = 4  # 4 is an arbitrary number you can play with these numbers
head_to_visualize = 0  # plot attention from this multi-head attention's head
gat_layer_id = 0  # plot attention from this GAT layer

assert gat_layer_id == 0, f'Attention visualization for {dataset_name} is only available for the first layer.'

# Build up the complete graph
# node_features shape = (N, FIN), where N is the number of nodes and FIN number of input features
total_num_of_nodes = len(all_nodes_unnormalized_scores)
complete_graph = ig.Graph()
complete_graph.add_vertices(total_num_of_nodes)  # igraph creates nodes with ids [0, total_num_of_nodes - 1]
edge_index_tuples = list(zip(edge_index[0, :], edge_index[1, :]))  # igraph requires this format
complete_graph.add_edges(edge_index_tuples)

target_node_ids = edge_index[1]
source_nodes = edge_index[0]

#
# Pick the node id you want to visualize the attention for!
#

# since for loops won't work with igraph just set some number here
target_node_id = 0  # random node

# Step 1: Find the neighboring nodes to the target node
# Note: self edges are included so the target node is it's own neighbor (Alexandro yo soy tu madre)
src_nodes_indices = torch.eq(target_node_ids, target_node_id)
source_node_ids = source_nodes[src_nodes_indices].cpu().numpy()
size_of_neighborhood = len(source_node_ids)

# Step 2: Fetch their labels
labels = node_labels[source_node_ids].cpu().numpy()

# Step 3: Fetch the attention weights for edges (attention is logged during GAT's forward pass above)
# attention shape = (N, NH, 1) -> (N, NH) - we just squeeze the last dim it's superfluous
all_attention_weights = gat.gat_net[gat_layer_id].attention_weights.squeeze(dim=-1)
print(src_nodes_indices)
attention_weights = all_attention_weights[src_nodes_indices, head_to_visualize].cpu().numpy()
# PPI's attention pattern is much less uniform than Cora's
print(f'Max attention weight = {np.max(attention_weights)} and min = {np.min(attention_weights)}')
attention_weights /= np.max(attention_weights)  # rescale the biggest weight to 1 for nicer plotting

# Build up the neighborhood graph whose attention we want to visualize
# igraph constraint - it works with contiguous range of ids so we map e.g. node 497 to 0, 12 to 1, etc.
id_to_igraph_id = dict(zip(source_node_ids, range(len(source_node_ids))))
ig_graph = ig.Graph()
ig_graph.add_vertices(size_of_neighborhood)
ig_graph.add_edges([(id_to_igraph_id[neighbor], id_to_igraph_id[target_node_id]) for neighbor in source_node_ids])

# Prepare the visualization settings dictionary and plot
visual_style = {
    "edge_width": attention_weights,  # make edges as thick as the corresponding attention weight
    "layout": ig_graph.layout_reingold_tilford_circular()  # layout for tree-like graphs
}

ig.plot(ig_graph, **visual_style)

"""Beautiful! üéâüòç

Compared to Cora the attention patterns here are not constant!

There is one more way to understand that GAT is learning interesting attention patterns, and that brings us to entropy histograms!

# Visualizing entropy histograms üìä

So, I hear you say, wait `entropy`, what? How did [Claude Shannon](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) find his way in?

Well it's not that hard. The attention coefficients sum up to 1 - they form a probability distribution. Where there is a probability distribution you can calculate the entropy. And the entropy quantifies the amount of information in a distribution (for my uber geeks - it's an expected value of self-information ü§ì).

Check out this [amazing video](https://www.youtube.com/watch?v=ErfnhcEV1O8) if you're not familiar with the concept of entropy, but actually you don't need to understand the theory of entropy so much in order to understand why we're doing this.

The main idea is the following:

If we have a **"hypothetical" GAT** that has a const attention over every node's neighborhood (i.e. **all distributions are uniform**), and we calculate the entropy (whatever that may be) of each and every neighborhood, and we make a histogram out of those numbers - **how different are the histograms** coming from it compared to the GAT we just trained?

If the answer is they completely overlap, well that means our GAT has got uniform attention patterns. The smaller the overlap the less uniform the distributions are. We don't care about the information, per se, we care about how much the histograms **match**.

Helpfully that brings some clarity into your mind. With that out of the way let's define a couple of functions we'll need:
"""

# Draws (but doesn't yet plot) the entropy histogram. If you're confused to why do we have entropy here all of a sudden
# bear with me you'll soon understand. Basically it helps us quantify the usefulness of GAT's learned attention pattern.
def draw_entropy_histogram(entropy_array, title, color='blue', uniform_distribution=False, num_bins=30):
    max_value = np.max(entropy_array)
    bar_width = (max_value / num_bins) * (1.0 if uniform_distribution else 0.75)
    histogram_values, histogram_bins = np.histogram(entropy_array, bins=num_bins, range=(0.0, max_value))

    plt.bar(histogram_bins[:num_bins], histogram_values[:num_bins], width=bar_width, color=color)
    plt.xlabel(f'entropy bins')
    plt.ylabel(f'# of node neighborhoods')
    plt.title(title)

from scipy.stats import entropy


# Let's define an enum as a clean way to pick between different visualization options
class VisualizationType(enum.Enum):
    ATTENTION = 0,
    ENTROPY = 1


def visualize_entropy_histograms(model_name=r'gat_PPI_000000.pth', dataset_name=DatasetType.PPI.name):
    # Fetch the data we'll need to create visualizations
    all_nodes_unnormalized_scores, edge_index, node_labels, gat = gat_forward_pass(model_name, dataset_name)

    # We want our local probability distributions (attention weights over the neighborhoods) to be
    # non-uniform because that means that GAT is learning a useful pattern. Entropy histograms help us visualize
    # how different those neighborhood distributions are from the uniform distribution (constant attention).
    # If the GAT is learning const attention we could well be using GCN or some even simpler models.
    num_heads_per_layer = [layer.num_of_heads for layer in gat.gat_net]
    num_layers = len(num_heads_per_layer)
    num_of_nodes = len(node_features)

    target_node_ids = edge_index[1].cpu().numpy()

    # For every GAT layer and for every GAT attention head plot the entropy histogram
    for layer_id in range(num_layers):
        # Fetch the attention weights for edges (attention is logged during GAT's forward pass above)
        # attention shape = (N, NH, 1) -> (N, NH) - we just squeeze the last dim it's superfluous
        all_attention_weights = gat.gat_net[layer_id].attention_weights.squeeze(dim=-1).cpu().numpy()

        # tmp fix for PPI there are some numerical problems and so most of attention coefficients are 0
        # and thus we can't plot entropy histograms
        if dataset_name == DatasetType.PPI.name and layer_id > 0:
            print(f'Entropy histograms for {dataset_name} are available only for the first layer.')
            break

        for head_id in range(num_heads_per_layer[layer_id]):
            uniform_dist_entropy_list = []  # save the ideal uniform histogram as the reference
            neighborhood_entropy_list = []

            # This can also be done much more efficiently via scatter_add_ (no for loops)
            # pseudo: out.scatter_add_(node_dim, -all_attention_weights * log(all_attention_weights), target_index)
            for target_node_id in range(num_of_nodes):  # find every the neighborhood for every node in the graph
                # These attention weights sum up to 1 by GAT design so we can treat it as a probability distribution
                neigborhood_attention = all_attention_weights[target_node_ids == target_node_id].flatten()
                # Reference uniform distribution of the same length
                ideal_uniform_attention = np.ones(len(neigborhood_attention))/len(neigborhood_attention)

                # Calculate the entropy, check out this video if you're not familiar with the concept:
                # https://www.youtube.com/watch?v=ErfnhcEV1O8 (Aur√©lien G√©ron)
                neighborhood_entropy_list.append(entropy(neigborhood_attention, base=2))
                uniform_dist_entropy_list.append(entropy(ideal_uniform_attention, base=2))

            title = f'{dataset_name} entropy histogram layer={layer_id}, attention head={head_id}'
            draw_entropy_histogram(uniform_dist_entropy_list, title, color='orange', uniform_distribution=True)
            draw_entropy_histogram(neighborhood_entropy_list, title, color='dodgerblue')

            fig = plt.gcf()  # get current figure
            plt.show()
            fig.savefig(os.path.join(DATA_DIR_PATH, f'layer_{layer_id}_head_{head_id}.jpg'))
            plt.close()

"""Finally, let's run it!"""

visualize_entropy_histograms(
        model_name,
        dataset_name,
)

"""And voil√†, the light blue histograms (trained GAT) are skewed compared to the orange one (uniform attention GAT). And additionally, they are skewed to the left which we could have expected since the **uniform distributions have the highest entropy.**

If the previous visualization with edge thickness plotted didn't convince you I'm sure that entropy will! (*laughs in kilo bits per cringe*)

The idea for this visualization came from [this blog post](https://www.dgl.ai/blog/2019/02/17/gat.html) recommended to me by Petar Veliƒçkoviƒá.

---

Phew!!! That was a mouthful! If you stayed with me until here, **congrats!** (achievement unlocked - GAT master üòç)

Take your time to analyze this notebook. This is not a toy project, it took me ~3 weeks to finish it <br/> 
so don't expect to understand everything in 30 minutes unless you're really familiar with most of the concepts mentioned here.

And last but not least!

# Connect with me

I share lots of useful (I hope so at least!) content on LinkedIn, Twitter, YouTube and Medium. <br/>
So feel free to connect with me there:
1. My [LinkedIn](https://www.linkedin.com/in/aleksagordic) and [Twitter](https://twitter.com/gordic_aleksa) profiles
2. My YouTube channel - [The AI Epiphany](https://www.youtube.com/c/TheAiEpiphany)
3. My [Medium](https://gordicaleksa.medium.com/) profile

Also do drop me a message if you found this useful or if you think I could've done something better! <br/>
I always like getting some feedback on the work I do.

If you notice some bugs/errors feel free to **open up an issue** or even **submit a pull request**.

# Additional resources

If you're interested in learning more about GNNs there are many awesome resources out there.

* Check out [Sergey Ivanov's newsletter](https://graphml.substack.com/p/issue-1-introduction-pac-isometry-over-smoothing-and-evolution-of-the-field-265283)
* [Michael Bronstein's](https://medium.com/@michael.bronstein) blog posts

And of course watch my videos:
* [My overview of the GCN paper](https://www.youtube.com/watch?v=VyIOfIglrUM)
* [My overview of the GraphSAGE paper](https://www.youtube.com/watch?v=vinQCnizqDA)
* [My overview of the PinSage paper](https://www.youtube.com/watch?v=ed0NJdqwEyg)
* [My overview of Temporal Graph Networks (TGN)](https://www.youtube.com/watch?v=0tw66aTfWaI)

Also, follow GNN experts on Twitter. That's a good strategy to stay in touch with the field. Check out the people I follow that's better than me cherry-picking some names here.

*Note: I didn't do it justice by only linking Michael and Sergey - I'll probably create a video where I'll explain how I approached learning about GNNs and various different resources I leveraged.*
"""

i = torch.tensor(0.0).cuda()
import time
while True:
    i *= i
    time.sleep(10.0)